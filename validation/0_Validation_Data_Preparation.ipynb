{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & Novel Data Generation\n",
    "\n",
    "As all the 3 models we created were trained with a seemingly high accuracy, we then proceeded to evaluate novel data points using those models. For the same, we generated and evaluated ~750K Novel Points using QGIS. The step by step guide is mentioned in this notebook. \n",
    "\n",
    "Also, you can find all the files that we create in this notebook [here](https://drive.google.com/drive/folders/1BRlfZGzUpyTjIrAThURLgLkGEeEuDDiu?usp=sharing).\n",
    "\n",
    "\n",
    "So without any further ado, let's start!\n",
    "\n",
    "1. In QGIS, we opened the smallest raster by size - `SA_TMI` - to get the boundary.\n",
    "2. Then, we plotted the ~415,000 known data-points as Delimitted Text File over the raster. (`unsampled.csv` file - the one used for training of model 2). _Note - The CRS of the ratser file was changed to EPSG 4326 by clicking on the Bottom right button to match that of the CSV file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Then, by using the **Buffer Tool**, we create a circular shape arround all points. This will create a Shape file for a polygon from which we will then generate the novel points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Don't Change the default values in the dialogue box till neccesary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Then by using the `Polygon Vector Tool`, we created a polygon encapsulating the raster's full boundries.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Then, by using the `Difference Tool`, we created a polygon for the Area that didnt had any plotted mineral locations. The resulting polygon was saved as `AreaUnexplored.gpkg`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](res/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Then, by using the `Random Point inside polygon Tool`, we created a vector Layer with 1 Million point lying inside the `AreaUnexplored` polygon.\n",
    "\n",
    "7. We then saved this layer as a Delimitted Text File - `validation_unsampled.csv` for further sampling and validation. You can find the sa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The generated points seem to overlap but when you zoom in, you'll find that they are spaced apart.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](res/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the `validation_unsampled.csv`, we will now have to sample these points via QGIS to obtain the `validation_sampled.csv`. This csv will contain 1 million points, from which after dropping NA's, we obtain a final csv file with `~723K points`. This file was renamed to `Validation_Model_1.csv`.\n",
    "\n",
    "This csv file is then used to obtain predictions from the first model. The predictions are saved as `Model_1_Predictions.csv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we'll have a look about how the predictions of Model 2 & Model 3 are obtained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
